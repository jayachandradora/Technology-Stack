

# Kafka SASL Mechanisms & Security Protocol

---

# âœ… 1. SASL Mechanisms for Producing Messages to Kafka

Kafka supports several **SASL (Simple Authentication and Security Layer)** mechanisms for authentication between client (producer/consumer) and broker.

### **Common SASL Mechanisms in Kafka**

| Mechanism                             | Description                                                     | When Used                           |
| ------------------------------------- | --------------------------------------------------------------- | ----------------------------------- |
| **PLAIN**                             | Username/password in plain text (usually with SSL to secure it) | Simple auth, non-enterprise systems |
| **SCRAM-SHA-256** / **SCRAM-SHA-512** | Secure salted password hashing                                  | More secure alternative to PLAIN    |
| **GSSAPI (Kerberos)**                 | Kerberos-based authentication                                   | Enterprise AD/kerberos environments |
| **OAUTHBEARER**                       | OAuth 2.0 / OIDC tokens                                         | Cloud/Kafka SaaS/auth providers     |
| **AWS_MSK_IAM**                       | IAM-based authentication (for AWS MSK)                          | AWS MSK only                        |

---

# âœ… 2. SASL_OAUTHBEARER for Kafka Producers

When using **OAUTHBEARER**, the Kafka producer obtains a **JWT or access token** from an OAuth/OIDC provider (Keycloak, Okta, Auth0, etc.) and presents it to the Kafka broker during authentication.

### Example Producer Config for OAUTHBEARER

```properties
security.protocol=SASL_SSL
sasl.mechanism=OAUTHBEARER
sasl.login.callback.handler.class=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginCallbackHandler
sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
    oauth.token.endpoint.uri="https://auth.example.com/oauth2/token" \
    oauth.client.id="kafka-producer" \
    oauth.client.secret="my-secret" \
    oauth.scope="kafka.write";
```

### What happens internally

1. **Producer uses the login callback handler** to get a token.
2. Kafka client sends token â†’ Broker verifies token (JWT signature / introspection).
3. If valid, connection is authenticated.

---

# âœ… 3. `SASL_JAAS_CONFIG` (or `sasl.jaas.config`) Explained

`SASL_JAAS_CONFIG` is where you provide **credentials** for the SASL mechanism.

It always contains a Java login module.

### Example for SASL/PLAIN

```properties
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
  username="producer" \
  password="secret";
```

### Example for SCRAM

```properties
sasl.mechanism=SCRAM-SHA-512
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
  username="scramUser" \
  password="scramPwd";
```

### Example for OAuth

```properties
sasl.mechanism=OAUTHBEARER
sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
  oauth.client.id="clientId" \
  oauth.client.secret="clientSecret";
```

ğŸ‘‰ **It tells Kafka which LoginModule to use and with what config.**

---

# âœ… 4. `sasl.login.callback.handler.class` Explained

This property points to the **class that performs authentication token retrieval**.

Kafka uses callback handlers for mechanisms that require dynamic actions such as:

* Generating OAuth tokens
* Refreshing tokens
* Interacting with Kerberos

### Built-In Handler for OAUTHBEARER

```properties
sasl.login.callback.handler.class=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginCallbackHandler
```

This class:

* contacts OAuth server
* gets access token
* caches token
* refreshes token on expiration

### For Custom Token Generation (e.g., your own JWT service)

```properties
sasl.login.callback.handler.class=com.mycompany.kafka.CustomBearerTokenHandler
```

You implement:

```java
public class CustomBearerTokenHandler
        implements AuthenticateCallbackHandler {
    @Override
    public void handle(Callback[] callbacks) {
        // Your logic to fetch / generate a JWT
    }
}
```

---

# âœ… 5. Summary Table

| Setting                             | Purpose                                                |
| ----------------------------------- | ------------------------------------------------------ |
| `sasl.mechanism`                    | Which authentication method Kafka should use           |
| `sasl.jaas.config`                  | Credentials or configuration needed for that mechanism |
| `sasl.login.callback.handler.class` | Custom logic for producing tokens (OAuth)              |

---

# Apache Kafka, the security.protocol

In Apache Kafka, the **`security.protocol`** setting defines *how* the client (producer/consumer/admin) connects to the brokersâ€”specifically what security layer is used.

For producing messages, you can use any of the following **valid `security.protocol` values**:

---

## âœ… **Kafka `security.protocol` Options**

### **1. `PLAINTEXT`**

* No authentication
* No encryption
* **Not secure**, only use in internal/trusted networks

---

### **2. `SSL`**

* TLS encryption
* Optional client certificates for mutual auth
* Good for encrypted communication without SASL

---

### **3. `SASL_PLAINTEXT`**

* SASL authentication
* No encryption
* Use only in trusted networks
* With SASL mechanisms like:

  * `PLAIN`
  * `SCRAM-SHA-256`
  * `SCRAM-SHA-512`
  * `GSSAPI` (Kerberos)
  * `OAUTHBEARER`

---

### **4. `SASL_SSL`**  â­ *Most common secure option*

* **TLS encryption + SASL authentication**
* Recommended for production
* Supports the same SASL mechanisms as above
* Example:

  ```properties
  security.protocol=SASL_SSL
  sasl.mechanism=SCRAM-SHA-512
  ```

---

## Summary Table

| security.protocol | Encryption | Authentication  | Notes         |
| ----------------- | ---------- | --------------- | ------------- |
| `PLAINTEXT`       | âŒ          | âŒ               | No security   |
| `SSL`             | âœ…          | Optional (mTLS) | TLS only      |
| `SASL_PLAINTEXT`  | âŒ          | âœ… (SASL)        | Auth only     |
| `SASL_SSL`        | âœ…          | âœ… (SASL)        | Best practice |

---

## Example Producer Config Using SASL_SSL (SCRAM)

```properties
bootstrap.servers=broker:9093
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-512
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
  username="user" \
  password="password";
```

---

# Kafka key deserializers and value deserializers

Below is a **comprehensive, structured, and best-practiceâ€“focused explanation** of **Kafka key deserializers and value deserializers**, including all commonly used types, how they work, configuration details, design considerations, and expert patterns used in production systems.

---

# âœ… **1. What Are Kafka Deserializers?**

Kafka producers **serialize** data into bytes before sending them to Kafka.
Kafka consumers must then **deserialize** these bytes back into objects.

* **key.deserializer** â†’ converts *record key bytes* into a Java/Python/etc. object
* **value.deserializer** â†’ converts *record value bytes* into an object

They're both specified in consumer configuration.

### Example (Java):

```java
props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
```

---

# âœ… **2. All Common Kafka Deserializers (Built-In)**

These are provided by **Apache Kafka** out of the box.

| Deserializer               | Class Name                                                     | Description                            |
| -------------------------- | -------------------------------------------------------------- | -------------------------------------- |
| **StringDeserializer**     | `org.apache.kafka.common.serialization.StringDeserializer`     | Converts bytes â†’ UTF-8 string          |
| **IntegerDeserializer**    | `org.apache.kafka.common.serialization.IntegerDeserializer`    | Converts bytes â†’ Java Integer          |
| **LongDeserializer**       | `org.apache.kafka.common.serialization.LongDeserializer`       | Converts bytes â†’ Java Long             |
| **FloatDeserializer**      | `org.apache.kafka.common.serialization.FloatDeserializer`      | Converts bytes â†’ Java Float            |
| **DoubleDeserializer**     | `org.apache.kafka.common.serialization.DoubleDeserializer`     | Converts bytes â†’ Double                |
| **ByteArrayDeserializer**  | `org.apache.kafka.common.serialization.ByteArrayDeserializer`  | Keeps raw byte[] as is                 |
| **ByteBufferDeserializer** | `org.apache.kafka.common.serialization.ByteBufferDeserializer` | Converts bytes â†’ ByteBuffer            |
| **BytesDeserializer**      | `org.apache.kafka.common.serialization.BytesDeserializer`      | Converts bytes â†’ Kafka `Bytes` wrapper |
| **UUIDDeserializer**       | `org.apache.kafka.common.serialization.UUIDDeserializer`       | Converts 16-byte data â†’ UUID           |

---

# âœ… **3. Third-party / ecosystem deserializers**

### â­ Confluent Schema Registry Deserializers

If using Avro / Protobuf / JSON-Schema, these are the industry standard.

| Format          | Deserializer                                                        |
| --------------- | ------------------------------------------------------------------- |
| **Avro**        | `io.confluent.kafka.serializers.KafkaAvroDeserializer`              |
| **Protobuf**    | `io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializer` |
| **JSON Schema** | `io.confluent.kafka.serializers.json.KafkaJsonSchemaDeserializer`   |

---

### â­ JSON Deserializers

Not built into Kafka.
Common sources:

| Library        | Deserializer                      |
| -------------- | --------------------------------- |
| Spring Kafka   | `JsonDeserializer`                |
| Jackson custom | custom class using `ObjectMapper` |
| Gson           | custom class using `Gson`         |

---

### â­ Custom Deserializers

You can implement your own by writing:

```java
public class MyCustomDeserializer implements Deserializer<MyType> {
    @Override
    public MyType deserialize(String topic, byte[] data) {
        // custom decoding logic
    }
}
```

Useful for:

* Complex domain objects
* Encrypted messages
* Custom binary formats
* Performance-optimized serializers (e.g., Kryo, FlatBuffers)

---

# âœ… **4. Detailed Explanation of Concepts**

## **4.1 Why Serialize/Deserialize?**

Kafka stores messages as raw bytes for:

* language independence
* high throughput
* low overhead

Deserialization allows consumers to convert these bytes into:

* Strings
* Numbers
* Avro records
* JSON POJOs
* Custom objects

---

## **4.2 Importance of Key Deserializers**

Keys are used for:

* Log compaction
* Partitioning
* Identifying unique entity streams (e.g., orders per orderId)

Typical key types:

* `String`
* `Long`
* `UUID`

---

## **4.3 Value Deserialization Strategy**

Values represent your payload.

Common choices:

* **String** â†’ simple messages / debugging
* **JSON** â†’ flexible, human-readable
* **Avro** (Schema Registry) â†’ enterprise standard
* **Protobuf** â†’ high performance, strongly typed
* **Custom binary** â†’ max performance, small footprint

---

# âœ… **5. Best Practices for Kafka Deserializers**

### **5.1 Use Schema-based Serialization (Recommended)**

If you are designing a production system:

âœ” **Avro or Protobuf + Schema Registry**
Why?

* Schema evolution support
* Backward compatibility
* Explicit typing
* Small & fast binary size
* Safe CI/CD deployments

---

### **5.2 Never Deserialize Untrusted Data Without Validation**

Kafka can contain:

* corrupt messages
* schema mismatches
* incompatible versions

Always guard against exceptions:

```java
try {
    return objectMapper.readValue(data, MyClass.class);
} catch (Exception e) {
    log.error("Bad data in topic {}", topic);
    return null; // or send to DLQ
}
```

---

### **5.3 Use Dead Letter Queues (DLQ)**

If deserialization fails:

* send message to a DLQ
* keep consumer healthy
* avoid blocking the entire partition

Spring Kafka example:

```yaml
error-handling.deserializer:
  fail-on-error: false
```

---

### **5.4 Keep Keys Simple**

Use:

* String
* UUID
* Long

Avoid:

* JSON keys
* Complex objects
* Huge payloads in keys (because they affect partitioning)

---

### **5.5 Use ByteArrayDeserializer When Payload is Large**

Avoid unnecessary CPU conversion.

---

### **5.6 For High Throughput Systems â†’ Use Binary Formats**

* Protobuf
* Avro
* FlatBuffers
* Kryo

These outperform JSON dramatically.

---

### **5.7 Always Align Serializer and Deserializer**

If producer uses:

```
KafkaAvroSerializer
```

Consumer must use:

```
KafkaAvroDeserializer
```

Mismatch = corruption.

---

### **5.8 Handle Null Keys / Null Values**

For deletes in compacted topics:

* key present
* value = null
  â†’ tombstone record

Deserializer must not break on null values.

---

# âœ… **6. Recommended Choices by Use Case**

| Use Case                     | Key Deserializer | Value Deserializer      |
| ---------------------------- | ---------------- | ----------------------- |
| Simple text messages         | String           | String                  |
| Logs / events                | String           | JSON                    |
| Enterprise microservices     | String / UUID    | Avro or Protobuf        |
| Financial data               | Long/UUID        | Avro or Protobuf        |
| High-frequency trading / IoT | Long             | Protobuf or FlatBuffers |
| Lightweight testing          | String           | String                  |
| Raw binary                   | ByteArray        | ByteArray               |

---

# âœ… **7. Example Configurations**

## **String Key + JSON Value (common)**

```java
props.put(KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
props.put(VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
```

---

## **UUID Key + Protobuf Value**

```java
props.put(KEY_DESERIALIZER_CLASS_CONFIG, UUIDDeserializer.class);
props.put(VALUE_DESERIALIZER_CLASS_CONFIG, KafkaProtobufDeserializer.class);
```

---

## **Long Key + Avro Value**

```java
props.put(KEY_DESERIALIZER_CLASS_CONFIG, LongDeserializer.class);
props.put(VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class);
```

---

# âœ… **8. Summary Checklist**

### âœ” What Kafka supports out of the box

* String
* Int/Long/Double
* UUID
* Byte[]
* ByteBuffer

### âœ” What enterprise applications use

* Avro
* Protobuf
* JSON
* Custom serializers

### âœ” Best overall strategy

ğŸš€ **Use Schema Registry + Avro/Protobuf** for compatibility & safety.

---

# If you want, I can also provide:

âœ… Sample Producer + Consumer Code (Java, Python, Node, Go)
âœ… Comparison table: Avro vs Protobuf vs JSON
âœ… A diagram of the Kafka serialization/deserialization pipeline
âœ… Best architecture pattern for your specific use case

Below are the **three items you requested**, crafted for clarity, interview readiness, and real-world architecture design.

---

# âœ… **1. Comparison Table: Avro vs Protobuf vs JSON (Kafka Serialization)**

This is the most complete, production-quality comparison.

| Feature                                 | **Avro**                                        | **Protobuf**                                      | **JSON**                                     |
| --------------------------------------- | ----------------------------------------------- | ------------------------------------------------- | -------------------------------------------- |
| **Serialization Format**                | Binary                                          | Binary                                            | Text                                         |
| **Schema Required?**                    | âœ” Yes                                           | âœ” Yes                                             | âŒ Optional                                   |
| **Schema Registry Support**             | âœ” Strong (Confluent standard)                   | âœ” Strong                                          | âœ” JSON Schema                                |
| **Schema Evolution**                    | âœ” Excellent                                     | âœ” Excellent                                       | âš  Partial, fragile                           |
| **Encoding Size**                       | Small                                           | Smallest                                          | Large                                        |
| **Performance (speed)**                 | Fast                                            | Fastest                                           | Slow                                         |
| **Human-readable?**                     | âŒ No                                            | âŒ No                                              | âœ” Yes                                        |
| **IDL (Interface Definition Language)** | Avro IDL / JSON schema                          | `.proto` files                                    | JSON Schema (optional)                       |
| **Backward / Forward Compatibility**    | âœ” Very strong                                   | âœ” Very strong                                     | âš  Poor without strict schema discipline      |
| **Tooling Ecosystem**                   | Great (Confluent + Hadoop ecosystem)            | Excellent (Google ecosystem)                      | Universal but loose                          |
| **Best Use Cases**                      | Event-driven microservices, analytics pipelines | High-performance microservices, mobile/cloud APIs | Low-volume logs, debugging, flexible schemas |
| **Kafka Enterprise Standard**           | â­ Most popular                                  | â­ Gaining popularity                              | Only for simple systems                      |
| **Learning Curve**                      | Medium                                          | Medium-High                                       | Low                                          |

### **Final Verdict**

* **If you're on Kafka â†’ Avro or Protobuf.**
* JSON is only good for:

  * debugging
  * very small workloads
  * human-readable logs

---

# âœ… **2. Diagram: Kafka Serialization/Deserialization Pipeline**

Below is a clear text-based diagram (ASCII-style) that you can convert into architecture docs or presentations.

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚        Producer App         â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                     (1) Create Object
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚       Serializer            â”‚
                 â”‚ (Avro/Proto/JSON/String)   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                     (2) Convert to Bytes
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚      Kafka Broker           â”‚
                 â”‚ Stores only raw byte[]      â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                     (3) Consume Bytes
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚      Deserializer           â”‚
                 â”‚ (Avro/Proto/JSON/String)   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                     (4) Convert to Object
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚       Consumer App          â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

If using Schema Registry:

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    Schema Registry   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
          (Serializer gets ID, Deserializer fetches schema)
```

**Key principle:**
â¡ Kafka stores *bytes only*. Schema Registry keeps your *schema & evolution safety*.

---

# âœ… **3. Best Architecture Pattern for Your Use Case**

Below is the **industry-proven architecture** for modern Kafka microservices using schemas safely.

---

## â­ **Recommended Architecture Pattern: â€œSchema-Driven Event Streamingâ€**

This pattern ensures:

* backward/forward compatibility
* safe schema versioning
* microservices independence
* evolvable interfaces

---

## **Architecture Overview**

```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚        Producer Service       â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ 1. Build Event (POJO)
                                    â”‚
                                    â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚     Avro/Protobuf Serializer  â”‚
                     â”‚   (Registers schema if new)   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ 2. Serialize + Schema ID
                                    â”‚
                                    â–¼
                            Kafka Topic
                                    â”‚
                                    â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚     Consumer Service          â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ 3. Consume Bytes + Schema ID
                                    â”‚
                                    â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Avro/Protobuf Deserializer    â”‚
                     â”‚  (Fetch schema by ID)         â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ 4. Build POJO
                                    â”‚
                                    â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Business Logic Processing   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â­ **Components of the Pattern**

### **1. Use Schema Registry**

Mandatory for:

* versioning
* compatibility rules (backward/forward/full)
* safe deployments

### **2. Prefer Avro or Protobuf Over JSON**

Reasons:

* version-safe
* high performance
* strict types

### **3. Produce Only â€œEvents,â€ Never Models**

The event schema is *independent* of your serviceâ€™s internal class structure.

### **4. Use Keyed Messages**

Keys should be:

* **String**, **UUID**, **Long**

Keys drive:

* partitioning
* ordering guarantees
* idempotent processing

### **5. Use DLQs (Dead-Letter Queues)**

For:

* deserialization failures
* business logic failures
* poison messages

### **6. Use Consumer Groups for Parallelism**

Each microservice horizontally scales automatically.

### **7. Donâ€™t Break Compatibility**

Set Schema Registry mode:

* **BACKWARD** (recommended)

This allows:

* new consumer reads old messages
* old consumer reads new messages (if non-breaking)

---

# â­ Summary of the Recommended Stack

### **Producer**

* Avro or Protobuf
* Schema Registry (Confluent)
* String/Long/UUID keys

### **Kafka Broker**

* Raw byte storage
* Retention, compaction as configured

### **Consumer**

* Avro or Protobuf
* Schema Registry-aware deserializer
* DLQ support

### **Benefit**

âœ” Safe schema evolution
âœ” Strong contracts
âœ” Scalable microservices
âœ” High throughput
âœ” Zero downtime deployments


